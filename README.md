# ğŸ–ï¸ **Real-Time Sign Language to Speech Translator**

This project captures real-time sign language gestures (Aâ€“Z) using a webcam, classifies them with a trained Deep Learning model (CNN + LSTM), and converts the output into speech. The system uses Mediapipe for hand detection and runs via a Flask web interface with live video streaming.

## âš™ï¸ Installation

Clone the repository
```
git clone https://github.com/swalihopc/Real-Time-Sign-Language-to-Speech-Translator.git
cd Real-Time-Sign-Language-to-Speech-Translator
```

Create a virtual environment
```
python -m venv .venv
.venv\Scripts\activate      # Windows
source .venv/bin/activate   # Linux/Mac
```

Install dependencies
```
pip install -r requirements.txt
```

## ğŸ“Š Dataset

- Dataset must be organized into subfolders for each letter (Aâ€“Z):

- Use collectdata.py to capture new gesture images via webcam.

- Each key press (Aâ€“Z) saves an image into the respective folder.

## ğŸ§  Training

- Load dataset & preprocess with data.py and function.py

- Extract hand landmarks using Mediapipe

- Train a CNN + LSTM model

- Save trained model as sign_language_model.h5

## ğŸš€ Running the Translator

Start the Flask app:
```
python app.py
```

Open your browser at:
ğŸ‘‰ http://127.0.0.1:5000/

You will see:
âœ… Live webcam feed
âœ… Bounding box on detected hand
âœ… Predicted letter overlay with confidence score
âœ… Speech output of recognized letters

## ğŸ“Œ Future Enhancements

- Recognize words & sentences, not just letters

- Improve accuracy with larger datasets

- Deploy as a mobile/desktop app

- Add multi-language text-to-speech
